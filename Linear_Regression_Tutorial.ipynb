{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3RGDMmfcWOKiN0tXvDr6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkanh6/CMP_SC-7540-Neural-Models/blob/main/Linear_Regression_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression Tutorial**"
      ],
      "metadata": {
        "id": "n8i5iUgK_sDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Linear Regression?\n",
        "\n",
        "Supervised learning operates on the principle that both the input features (often denoted as **X**) and the corresponding outputs (denoted as **Y**) are known and labeled. This labeling allows the construction of a predictive model which, after training, can forecast outputs for new, unseen inputs. **Linear Regression** is a foundational statistical method in this domain that is used to predict a continuous numerical value. Unlike classification models which predict discrete categories, linear regression is designed for problems where the outcome could be any numerical value within a continuous range.\n",
        "\n",
        "# How Does the Model Determine the Best Line to Fit the Data?\n",
        "\n",
        "Linear regression models seek the line that best fits the training data, which in two-dimensional space is represented as `y = mx + b`. This line is called the hypothesis function. Here, `m` stands for the slope of the line, and `b` represents the y-intercept, the point at which the line crosses the y-axis. The \"best fit\" is achieved by minimizing the cost function, typically the mean squared error between the predicted values and the actual values in the training set. This process is known as *Linear Least Squares*.\n",
        "\n",
        "- **Hypothesis Function**:\n",
        "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1x $$\n",
        "\n",
        "- **Cost Function**:\n",
        "$$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 $$\n",
        "\n",
        "Where:\n",
        "- $\\theta_0$ is the y-intercept (b in the simple equation),\n",
        "- $\\theta_1$ is the slope (m in the simple equation),\n",
        "- $(x^{(i)}, y^{(i)})$ are the training examples,\n",
        "- $h_{\\theta}(x^{(i)})$ is the prediction for training example $x^{(i)}$,\n",
        "- $m$ is the number of training examples.\n",
        "\n",
        "The model's parameters are adjusted during training using optimization algorithms like Gradient Descent to find the values of $\\theta_0$ and $\\theta_1$ that minimize the cost function $J(\\theta_0, \\theta_1)$.\n",
        "\n",
        "\n",
        "# Terminology\n",
        "\n",
        "- **Training set** - Data used to train the model.\n",
        "- **x** - input variable or feature.\n",
        "- **y** - the \"output\" variable.\n",
        "- **m** - the number of training examples.\n",
        "- **(x,y)** - single training example.\n",
        "- **(x^{(i)}, y^{(i)})** - the i-th training example in the training set."
      ],
      "metadata": {
        "id": "pLu_xjloAEb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Linear Regression Without a Library\n",
        "\n",
        "\n",
        "In this section, you will learn how to implement linear regression manually without using any external libraries. This will involve setting up the hypothesis and cost functions, followed by gradient descent optimization."
      ],
      "metadata": {
        "id": "AQtXoqm6HEDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Using the data from 2.4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Data\n",
        "x_data = [1, 4, 7, 10]\n",
        "y_data = [1.5, 3.5, 9, 8]\n",
        "alpha = 0.01\n",
        "iterations = 100;\n",
        "\n",
        "\n",
        "# Parameters Initialization\n",
        "w = 0;\n",
        "b = 0;\n",
        "\n",
        "# Gradient Descent\n",
        "for i in range(iterations)\n",
        "    # Calculate the hypothesis\n",
        "    y_pred = w * x_data + b\n",
        "\n",
        "    # Calculate the loss (Mean Squared Error)\n",
        "    loss = np.mean((y_pred - y_data)**2)\n",
        "\n",
        "    # Calculate Gradients\n",
        "    gradient_w = np.mean(2* (y_pred-y_data) * x_data)\n",
        "    gradient_b = np.mean(2 * (y_pred - y_data))\n",
        "\n",
        "    # Update Parameters\n",
        "    w = w - alpha * gradient_w\n",
        "    b = b - alpha * gradient_b\n",
        "\n",
        "plt.scatter\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iW-2N_jNHDyT"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}